---
header-includes:
- \usepackage{longtable}
- \usepackage[utf8]{inputenc}
- \usepackage[spanish]{babel}\decimalpoint
- \setlength{\parindent}{1.25cm}
- \usepackage{amsmath}
- \usepackage{xcolor}
- \usepackage{cancel}
- \usepackage{array}
- \usepackage{float}
- \usepackage{multirow}
output:
  pdf_document:
    number_sections: true
fontsize: 12pt
papersize: letter
geometry: margin = 1in
language: "es"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(knitr)
library(tidyverse)
library(FactoMineR)
library(factoextra)
```

\input{titlepage}
\thispagestyle{empty}
\tableofcontents

\newpage

\pagestyle{myheadings}
\setcounter{page}{3}

\section{Introducción}

El siguiente análisis se realiza con el propósito de mostrar la gran utilidad del análisis de componentes principales (PCA) en múltiples aplicaciones estadísticas robustas,
como la construcción e implementación de modelos de machine learning que requieren una gran cantidad de recursos computacionales. 

Para el desarrollo de este análisis, se remitirá a una base de datos relacionada con variantes rojas y blancas del vino portugués "Vinho Verde" \cite{kaggle-bd}, en la cual se encuentran
una serie de variables que pretenden medir la calidad del vino en cuestión desde diferentes aspectos, así como una variable respuesta u "output" que define una puntuación 
final de calidad.

\newpage

\subsection{Descripción de la base de datos}

Se presentan las variables de entrada a las cuales se les desea realizar el análisis de componentes principales, y la variable respuesta, que no se incluye en el PCA, pero se utilizará para la creación y validación de modelos:

\begin{table}[H]
\centering
\begin{tabular}{|>{\centering\arraybackslash}p{1cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{3cm}|>{\centering\arraybackslash}p{7cm}|>{\centering\arraybackslash}p{2.2cm}|}
\hline
\textbf{No} & \textbf{Nombre}         & \textbf{Nombre en BD}      & \textbf{Descripción} & \textbf{Tipo de variable} \\ \hline
1           & Acidez fija             & fixed.acidity        & Se refiere al conjunto de los ácidos naturales procedentes de la uva (tartárico, málico, cítrico y succínico) \cite{vino1}                   & Explicativa               \\ \hline
2           & Acidez volátil          & volatile.acidity     &  La cantidad de ácido acético en el vino, que en niveles demasiado altos puede provocar un sabor desagradable a vinagre                    & Explicativa               \\ \hline
3           & Ácido cítrico           & citric.acid          &  Encontrado en pequeñas cantidades, el ácido cítrico puede agregar 'frescura' y sabor a los vinos.                    & Explicativa               \\ \hline
4           & Azúcar residual         & residual.sugar       &  La cantidad de azúcar que queda después de que se detiene la fermentación, es raro encontrar vinos con menos de 1 gramo / litro                     & Explicativa               \\ \hline
5           & Cloruros                & chlorides            &  La cantidad de sal en el vino                    & Explicativa               \\ \hline
6           & Dióxido de azufre libre & free.sulfur.dioxide  &  La forma libre de SO2 existente en equilibrio entre el SO2 molecular (como gas disuelto) y el ion bisulfito                    & Explicativa               \\ \hline
7           & Dióxido de azufre total & total.sulfur.dioxide &  Cantidad de formas libres y ligadas de SO2; En bajas concentraciones, el SO2 es mayormente indetectable en el vino.                    & Explicativa               \\ \hline
8           & Densidad                & density              &   Densidad del agua, depende del alcohol y la cantidad de azúcar utilizada                  & Explicativa               \\ \hline
9           & pH                      & pH                   & Describe qué tan ácido o básico es un vino en una escala de 0 (muy ácido) a 14 (muy básico); la mayoría de los vinos tienen entre 3 y 4                     & Explicativa               \\ \hline
10          & Sulfatos                & sulphates            & Un aditivo para el vino que puede contribuir a los niveles de dióxido de azufre (SO2), que actúa como antimicrobiano                      & Explicativa               \\ \hline
11          & Alcohol                 & alcohol              & El porcentaje de contenido de alcohol del vino                    & Explicativa               \\ \hline
12          & Calidad                 & quality              & Variable de salida (basada en datos sensoriales, puntuación entre 0 y 10)                    & Respuesta                 \\ \hline
\end{tabular}
\end{table}

\subsection{¿Por qué realizar un análisis por componentes principales?}

Se pretende conseguir una reducción en la dimensionalidad de los datos, manteniendo la mayor cantidad de información retenida posible, para posteriormente disponer de estas a la hora de crear modelos de cualquier tipo.

Es claro que a la hora de crear modelos estadísticos y computacionales tales como redes neuronales, bosques aleatorios, clasificadores lineales, entre otros, el gasto computacional es alto. El uso de PCA permite realizar una \textbf{reducción en el número de variables conservando un alto porcentaje de la variabilidad de la información.} Al utilizar un menor número de variables y conservar gran parte de la variabilidad, se puede disminuir el gasto computacional de los modelos sin comprometer la calidad de los mismos. 

Es bueno advertir que conocer qué variables aportan mayor variabilidad permitiría que un experto en el tema pueda descubrir más información sobre el mecanismo intrínseco del proceso.




\newpage
\section{Informe técnico}

```{r PCA1, echo=F}
#Leyendo y partiendo la BD
wine <- read.csv("winequality-red.csv", header = T)
labels <- wine[, 12]
data <- wine[, -12]

#Partiendo los datos para train y test
n <- dim(data)[1]
set.seed(123)
train_index <- sample(1:n, floor(0.8*n))
train_wine <- data[train_index, ]
test_wine <- data[-train_index, ]

#labels
train_labels <- labels[train_index]
test_label <- labels[-train_index]

#Creando PCA
wine_pca <- PCA(train_wine, graph = F, ncp = 7)

#Valores propios del PCA
eigen_wine <- get_eigenvalue(wine_pca)

#Obteniendo los individuos
individuos <- get_pca_ind(wine_pca)
variables <- get_pca_var(wine_pca)

#Screeplot
eigen_plot <- fviz_eig(wine_pca, addlabels = T)
```


Para la realización del análisis por componentes principales, se utilizó la función `PCA()` de la librería `FactoMineR`. A continuación se muestran y se comentan los resultados obtenidos.

```{r PCA2, echo=F}
colnames(eigen_wine) <- c("Eigen valores", 
                          "Porcentaje de varianza",
                          "Porcentaje de varianza acumulada")
rownames(eigen_wine) <- c("CP1", "CP2", "CP3", "CP4", "CP5", "CP6",
                          "CP7", "CP8", "CP9", "CP10", "CP11")
kable(round(eigen_wine, 4), escape = F, longtable = T, booktab = T)

```

En la tabla anterior, se presentan los valores propios asociados a cada una de las componentes principales. La componente principal 1, junto con la componente principal 2, explican el `r round(eigen_wine[2, 3], 2)`\% de la variación de los datos. Para explicar el `r round(eigen_wine[7, 3], 2)`\% de la variación de los datos se requieren 7 componentes principales, esto implicaría una reducción de dimensionalidad de 11 a 7, perdiendo solo aproximadamente un 10 \% de información sobre la variabilidad de los datos. 

En los siguientes dos gráficos, se presenta la información previa visualmente: inicialmente se muestra el porcentaje de variabilidad que explica cada componente, ordenado de mayor a menor. 

Adicionalmente, se grafica el porcentaje de variabilidad acumulada, este último permite determinar visualmente cuántas componentes principales se requieren para explicar cierto porcentaje de variabilidad en los datos, además, permite dar una idea de cuánto es el aumento de variabilidad que se explica cuando se añade una componente principal.


```{r PCA3, echo=F, fig.height=9, fig.width=7, message=F}
#Plots (Varianza acumulada y porcentaje de varianza)
#Screeplot (Porcentaje de variabilidad)
por_var <- fviz_eig(wine_pca)

#Porcentaje acumulado
cum_por <- ggplot(mapping = aes(x = 1:11, y = eigen_wine[, 3])) +
  geom_line() + xlim(1,11) +
  scale_x_continuous(breaks= 1:11) +
  geom_segment(mapping = aes(x = c(1, 7), xend = c(7, 7),
                             y = c(90.8761, 0), 
                             yend = c(90.8761, 90.8761)), 
               color = "red", linetype = "dashed")

por_var <- ggpubr::ggpar(por_var, title = "Porcentaje de variabilidad",
              xlab = "Componente principal", 
              ylab = "Porcentaje (%)")

cum_por <- ggpubr::ggpar(cum_por, title = "Variabilidad acumulada",
              xlab = "Componente principal",
              ylab = "Porcentaje (%)",
              ggtheme = theme_minimal())
ggpubr::ggarrange(por_var, cum_por, ncol = 1, nrow = 2)
```

Teniendo en cuenta que, por definición, las componentes principales 1 y 2 son las que mayor variabilidad explican, se presenta la contribución de cada una de las variables a dichas componentes. Es bueno advertir que la contribución no es una medida de calidad, sino de magnitud. 

Para dar un ejemplo, se puede notar que relativo a la componente principal 1, las variables acidez fija, ácido cítrico, pH y densidad, tienen una contribución mayor a la promedio. 

```{r PCA4, echo=F}
#Plots de variables que mas contribuyen a las PCs
contrib_plot_pc1 <- fviz_contrib(wine_pca, choice = "var")
contrib_plot_pc2 <- fviz_contrib(wine_pca, choice = "var", axes = 2)

#Renombrando ejes y demas
contrib_plot_pc1 <- ggpubr::ggpar(contrib_plot_pc1,
  title = "Contribución de las variables a la\ncomponente principal 1", ylab = "Contribuciones (%)") 
contrib_plot_pc2 <-ggpubr::ggpar(contrib_plot_pc2,
  title = "Contribución de las variables a la\ncomponente principal 2", ylab = "Contribuciones (%)") 

ggpubr::ggarrange(contrib_plot_pc1, contrib_plot_pc2,
                  ncol = 2, nrow = 1)
```

Como se advirtió previamente, la contribución no es una medida de calidad. Por esta razón, es necesario utilizar una medida como los cosenos cuadrados. 

Gráficamente, una variable que esté más cerca al círculo unitario indica mayor contribución. Análogamente, una variable que forme un ángulo más pequeño con el eje respectivo de la componente principal indica mayor calidad. Esto, se traduce en que una calidad se puede considerar alta cuando el $\cos^2(\theta) \geq 0.9$.

En los siguientes círculos de correlación se ilustra gráficamente ambas medidas.


```{r PCA5, echo=F, fig.height=9, fig.width=7}
#Circulos de correlacion
cos2_circle <- fviz_pca_var(wine_pca, repel = T, 
                            col.var = "cos2",
                            gradient.cols = c("#00AFBB",
                                              "#E7B800",
                                              "#FC4E07")) 


contrib_circle <- fviz_pca_var(wine_pca, repel = T, 
                            col.var = "contrib",
                            gradient.cols = c("#00AFBB",
                                              "#E7B800",
                                              "#FC4E07"))

#Renombrando los ejes
cos2_circle <- ggpubr::ggpar(cos2_circle,
                             title = "Circulo de correlación",
                             xlab = "Componente principal 1",
                             ylab = "Componente principal 2",
                             legend.title = "Coseno cuadrado")

# Anotaciones cos2
cos2_circle_anot <- cos2_circle + 
    annotate(
    geom = "curve", x = 0.3, y = 0.9, xend = 0.55, yend = 1, 
    colour = "blue",
    curvature = -.3, arrow = arrow(length = unit(2, "mm"))
  ) + 
  annotate(geom = "text", x = 0.56, y = 0.9, label = "Alta contribución \n Alta calidad \n Respecto \n a la CP2", hjust = "left", size = 3) +
  annotate(
    geom = "curve", x = -0.4, y = 0.42, xend = -0.75, yend = 0.52, 
    colour = "blue",
    curvature = 0.1, arrow = arrow(length = unit(2, "mm"))
    ) +
  annotate(geom = "text", x = -0.51, y = 0.76, label = "Baja calidad \n respecto a \n ambas \n componentes ", hjust = "right", size = 3) + 
  annotate(
    geom = "curve", x = -0.75, y = -0.05, xend = -0.5, yend = -0.25, 
    colour = "blue",
    curvature = -.3, arrow = arrow(length = unit(2, "mm"))
  ) +
  annotate(geom = "text", x = -0.55, y = -0.56, label = "Alta calidad \n respecto a CP1. \n Calidad cercana \n a cero respecto a \n CP2", hjust = "center", size = 3)


contrib_circle <- ggpubr::ggpar(contrib_circle,
                             title = "Circulo de correlación",
                             xlab = "Componente principal 1",
                             ylab = "Componente principal 2",
                             legend.title = "Contribución")
ggpubr::ggarrange(cos2_circle_anot, contrib_circle, ncol = 1, nrow = 2)
```

A continuación, se muestra el mapeo en las componentes principales 1 y 2 para los 25 individuos con mayor coseno cuadrado y con mayor contribución, respectivamente. A cada individuo se le asigna un valor en una escala de color, correspondiende a la medida en cuestión. 

```{r PCA6, echo=F}
#Plots para individuos
cos2_ind <- fviz_pca_ind(wine_pca, col.ind = "cos2",
                         gradient.cols = c("#00AFBB",
                                           "#E7B800",
                                           "#FC4E07"), 
                         repel = T,
                         select.ind = list(cos2 = 25))

contrib_ind <- fviz_pca_ind(wine_pca, col.ind = "contrib",
                            gradient.cols = c("#00AFBB",
                                              "#E7B800",
                                              "#FC4E07"), 
                            repel = T,
                            select.ind = list(contrib = 25))

#Renombrando los ejes
cos2_ind <- ggpubr::ggpar(cos2_ind,
                             title = "Gráfico de individuos",
                             xlab = "Componente principal 1",
                             ylab = "Componente principal 2",
                             legend.title = "Coseno cuadrado")

#Anotaciones
cos2_ind_ano <- cos2_ind + 
                  geom_vline(xintercept = wine_pca[["ind"]][["coord"]]["147",1], color = "red", linetype = "dotted") +
                  geom_hline(yintercept = wine_pca[["ind"]][["coord"]]["147",2], color = "red", linetype = "dotted") +
                  annotate(
                    geom = "curve", 
                    x = wine_pca[["ind"]][["coord"]]["147",1] + 0.03, 
                    y = wine_pca[["ind"]][["coord"]]["147",2] - 0.03, xend = -2, yend = 1, 
                    colour = "blue",
                    curvature = -.3, arrow = arrow(length = unit(2, "mm"))
                  ) +
                  annotate(geom = "text", x = -2, y = 0.6, label = "Observación 147 \n en el nuevo \n sistema de \n coordenadas", hjust = "center", size = 3)

contrib_ind <- ggpubr::ggpar(contrib_ind,
                             title = "Gráfico de individuos",
                             xlab = "Componente principal 1",
                             ylab = "Componente principal 2",
                             legend.title = "Contribución")

#ggpubr::ggarrange(cos2_ind_ano, contrib_ind, ncol = 1, nrow = 2)
cos2_ind_ano
contrib_ind
```

```{r PCA7, echo=F}
#En stand by
biplot <- fviz_pca_biplot(wine_pca, repel = T, 
                col.var = "#2E9FDF",
                col.ind = "#696969")
```

En las siguientes tablas se presenta de forma numérica los resultados más relevantes para el PCA: 

```{r tabla_cor, echo=F}
#Nombres de columnas
colnames(variables$cor) <- c("CP1", "CP2", "CP3", "CP4", 
                             "CP5", "CP6", "CP7")
#Mostrar tabla
kable(round(variables$cor, 4), escape = F, longtable = T, 
      booktab = T, 
      caption = "Coordenadas")  %>%
      kable_styling(latex_options = c("hold_position", "repeat_header"))
```


```{r tabla_cos2, echo=F}
#Nombres de columnas
colnames(variables$cos2) <- c("CP1", "CP2", "CP3", "CP4", 
                              "CP5", "CP6", "CP7")
#Mostrar tabla
kable(round(variables$cos2, 4), escape = F, longtable = T, 
      booktab = T, 
      caption = "Cosenos cuadrados")  %>%
      kable_styling(latex_options = c("hold_position", "repeat_header"))
```

```{r tabla_contrib, echo=F}
#Nombres de columnas
colnames(variables$contrib) <- c("CP1", "CP2", "CP3", "CP4", 
                                 "CP5", "CP6", "CP7")
#Mostrar tabla
kable(round(variables$contrib, 4), escape = F, longtable = T, 
      booktab = T, 
      caption = "Contribuciones")  %>%
      kable_styling(latex_options = c("hold_position", "repeat_header"))
```

\newpage

```{r tabla_coord, echo=F}
#Nombres de columnas
colnames(variables$coord) <- c("CP1", "CP2", "CP3", "CP4", 
                                 "CP5", "CP6", "CP7")
#Mostrar tabla
kable(round(variables$coord, 4), escape = F, longtable = T, 
      booktab = T,
      caption = "Coordenadas")  %>%
      kable_styling(latex_options = c("hold_position", "repeat_header"))
```

```{r prohibido, echo=F, eval=F}
#NUNCA EJECUTAR NADA EN ESTE CHUNK
#Obteniendo PCs y valores propios respectivamente
#wine_pca <- PCA(data, graph = F, ncp = 154)

#Guardando wine_pca como objeto de R
#write_rds(wine_pca, "wine_pca.rds")

#Importando los individuos
# write.csv(individuos$coord, "PCA_train.csv", row.names = F)
# write.csv(train_wine, "train.csv", row.names = F)
# write.csv(matrix(train_labels, ncol = 1), 
#           "train_labels.csv", row.names = F)
# write.csv(test_wine, "test_wine.csv", row.names = F)
# write.csv(matrix(test_label, ncol = 1),
#           "test_labels.csv", row.names = F)
# 
# write.csv(predict(wine_pca, test_wine)$coord, "test_pca.csv", 
#           row.names = F)
```

\section{Ajuste de modelos y comparación}

Puesto que carece de sentido alguno realizar análisis de componentes principales 
sin llevar los resultados a alguna aplicación práctica, se decide usar un modelo 
de machine learning para predecir el nivel de la calidad del vino. Esto
se realiza a través de separar los datos en conjuntos de entrenamiento y prueba,
tanto en los datos en su espacio original como los transformados a través del
análisis de componentes principales para ajustar un modelo en cada espacio
con su respectivo conjunto de entrenamiento.

Dicho lo anterior, se hace necesario mencionar que el objetivo de esta sección no
es otro que comparar el rendimiento de los modelos de regresión de bosque 
aleatorio ajustados sobre los datos de entrenamiento en el sistema coordenado 
original y en el sistema coordenado de las componentes principales.

Ambos modelos fueron ajustados usando grid search k-fold cross validation; durante
este proceso se tomaron medidas de la raíz del error cuadrático medio (RMSE) de 
ambos modelos para posteriormente verificar la existencia o no de diferencias 
significativas entre el RMSE de estos.

Cabe aclarar que se emplearon métodos no paramétricos para contrastar las hipótesis plantedas, además, dado que se realizó k-fold cross validation, se
tienen valores del RMSE para conjuntos de entrenamiento y prueba gracias
a la naturaleza del proceso de validación utilizado.

A continuación se muestra el resultado de la distribución del estadístico de prueba Bootstrap.

```{r MODS, echo = F}
PCA.MOD <- read.csv("DatoscompPCA.csv")[, -1]
NORM.MOD <- read.csv("DatoscompNorm.csv")[, -1]
mean_train0 <- mean(PCA.MOD$mean_score_train) - mean(NORM.MOD$mean_score_train)
mean_test0 <- mean(PCA.MOD$mean_score_test) - mean(NORM.MOD$mean_score_test)
boot_mean_train <- c()
boot_mean_test <- c()
set.seed(314159)
for (i in 1:2000) {
  boot_mean_train[i] <- mean(sample(PCA.MOD$mean_score_train, 1920, replace = T)) - mean(sample(NORM.MOD$mean_score_train, 1920, replace = T))
  boot_mean_test[i] <- mean(sample(PCA.MOD$mean_score_test, 1920, replace = T)) - mean(sample(NORM.MOD$mean_score_test, 1920, replace = T))
}
pvalue_train <- 2*min(mean(boot_mean_train <= mean_train0),mean(boot_mean_train >= mean_train0))
pvalue_test <- 2*min(mean(boot_mean_test <= mean_test0),mean(boot_mean_test >= mean_test0))

histboot1 <- ggplot(data.frame(x = boot_mean_train), aes(x))+
  geom_histogram(binwidth = 5e-4, col = "black", fill = "cyan")+
  labs(x = "", y = "Frecuencia",
       title = "Histograma para la diferencia del RMSE medio en el conjunto de\nentrenamiento del k-fold cross validation")+
  geom_segment(data = data.frame(x = c(quantile(boot_mean_train, c(0.025, 0.975)), mean_train0),
                                 y = c(0,0,0), Clase = c("Confianza", "Confianza",
                                                       "Estadístico de prueba")),
               mapping = aes(x = x,
                             xend = x,
                             y = y, yend = c(600, 600, 600), color = Clase, linetype = Clase))+
  scale_colour_manual(values = c("black", "red"))
  
histboot2 <- ggplot(data.frame(x = boot_mean_test), aes(x))+
  geom_histogram(binwidth = 5e-4, col = "black", fill = "cyan")+
  labs(x = "", y = "Frecuencia",
       title = "Histograma para la diferencia del RMSE medio en el conjunto de\nprueba del k-fold cross validation")+
  geom_segment(data = data.frame(x = c(quantile(boot_mean_test, c(0.025, 0.975)), mean_test0),
                                 y = c(0,0,0), Clase = c("Confianza", "Confianza",
                                                       "Estadístico de prueba")),
               mapping = aes(x = x,
                             xend = x,
                             y = y, yend = c(600, 600, 600), color = Clase, linetype = Clase))+
  scale_colour_manual(values = c("black", "red"))


ggpubr::ggarrange(histboot1, histboot2, nrow = 2, ncol = 1)
```

Como se puede observar, el estadístico de prueba reside en el interior del
intervalo de confianza Bootstrap del 95%, por lo que no se rechaza la hipótesis de
que el RMSE  media de ambos modelos es el mismo para el conjunto de entrenamiento.

Se reportan los p-valores de dicha prueba.

```{r pvals, echo = F}
dfpvals <- data.frame(pvalor = c(pvalue_train, pvalue_test))
rownames(dfpvals) <- c("Entrenamiento", "Prueba")
dfpvals %>%
  kable(row.names = T, col.names = "P-valor", align = "c",
        booktab = T, longtable = T) %>%
  kable_styling(position = "center")
```

Dichos p-valores son mucho mayores que el nivel de significancia usado (5%),
lo que deja en evidencia la compatibilidad entre los datos que se obtuvieron 
y la hipótesis de que no hay diferencias significativas en el RMSE medio
de un modelo u otro. De hecho, el hecho de que los p-valores resulten
tan cercanos a 1 evidencia una fuerte concordancia entre la igualdad de las medias
de la distribución Bootstrap del estadístico de prueba.

Finalmente en esta sección se comparan ambos modelos cuando predicen sobre conjuntos de datos que no hicieron parte de su entrenamiento. Para esto se
toman la raíz del error cuadrático medio (RMSE), la desviación absoluta de 
la media (MAE) y la desviación absoluta porcentual de la media (MAPE).

\newpage

```{r measures, echo = F}
dfmeasures <- read.csv("Compfinal.csv", row.names = 1)

dfmeasures %>% round(4) %>%
  mutate(Porcentaje_de_error = paste(as.character(Porcentaje_de_error), "%",
                                     sep = "")) %>%
  kable(col.names = c("Modelo completo",
                      "Modelo componentes principales",
                      "Porcentaje de error"),
        row.names = T, align = "c", booktab = T,
        longtable = T) %>%
  kable_styling(position = "center")
```

Como se puede observar, el modelo completo obtiene valores más pequeños
en las medidas anteriormente descritas cuando predice sobre el conjunto de prueba, sin embargo el modelo ajustado usando los datos de prueba llevados al espacio
de las componentes principales también obtiene medidas muy bajas, de hecho
en todas ellas posee un error relativo porcentual no mayor a 6.4%, por lo que
presenta un rendimiento muy similar al modelo original.

Así que al considerar las conclusiones de la prueba Bootstrap (no existen
diferencias significativas entre los rendimientos del modelo al predecir)
y la tabla, se puede aseverar con total tranquilidad que el modelo ajustado
con los datos en el espacio de las componentes principales es igual de bueno
que el ajustado sobre los datos en su espacio original a pesar
de que el primero contenía un conjunto que explicaba el 90% de la variabilidad
total del conjunto original. Adicionalmente se tiene una ganancia a nivel de coste
computacional pues al tener menos columnas, el número de operaciones
que debe realizar el software para ajustar modelos es menor y por ende esto
desemboca en un proceso más rápido y eficiente.
\section{Conclusiones}

\begin{itemize}
\item En primer lugar se tiene una alta influencia de las variables relacionadas con la acidez a la hora de explicar la componente principal 1, dichas variables se encuentran correlacionadas  de manera fuerte con el pH, el cual es una medida de acidez. 

\item El pH es la variable con mejor calidad al momento de explicar la componente principal 1 porque es la que forma un angulo más pequeño con dicho eje, seguida de las variables de acidez; complementando lo descrito anteriormente.

\item Las variables relacionadas con dioxido de sulfuro explican son las que explican de mejor manera la componente principal 2, adicionalmente son aquellas con mejor calidad de representación en dicha componente porque el ángulo que forman con dicho eje es pequeño. Además están altamente correlacionadas.

\item 
\end{itemize}
\bibliographystyle{ieeetr}
\bibliography{bibliografia}